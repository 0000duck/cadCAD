{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cleanup -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '40G', 'conf': {'spark.driver.maxResultSize': '30G', 'spark.dynamicAllocation.enabled': 'true', 'spark.sql.execution.arrow.enabled': 'true', 'maximizeResourceAllocation': 'true'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Application using [Sparkmagic](https://github.com/jupyter-incubator/sparkmagic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '40G', 'conf': {'spark.driver.maxResultSize': '30G', 'spark.dynamicAllocation.enabled': 'true', 'spark.sql.execution.arrow.enabled': 'true', 'maximizeResourceAllocation': 'true'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"driverMemory\": \"40G\", \n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"30G\", \n",
    "        \"spark.dynamicAllocation.enabled\": \"true\", \n",
    "        \"spark.sql.execution.arrow.enabled\": \"true\", \n",
    "        \"maximizeResourceAllocation\": \"true\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1589588649708_0021</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-238.ec2.internal:20888/proxy/application_1589588649708_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-121.ec2.internal:8042/node/containerlogs/container_1589588649708_0021_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pyspark import Row\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "from cadCAD import configs\n",
    "from cadCAD.utils.sys_exec import to_spark_df, to_pandas_df\n",
    "from cadCAD.utils.jupyter import get_home_dir, set_write_path\n",
    "from cadCAD.configuration.utils import configs_as_objs, configs_as_dataframe, configs_as_dicts\n",
    "from cadCAD.engine import ExecutionMode, ExecutionContext, Executor\n",
    "from cadCAD.engine.execution import distributed_simulations\n",
    "# from cadCAD.engine.execution import dist_simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Create / Encode System Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user = 'jovyan'\n",
    "sc.addPyFile(get_home_dir(user)+\"distroduce.zip\")\n",
    "\n",
    "# from distroduce.session import sc_alt as sc\n",
    "# from distroduce.session import spark_alt as spark\n",
    "from distroduce.engine import transform\n",
    "from distroduce.reggression_tests.models import sweep_config\n",
    "# System Model Intervention Point for RAD and Integration\n",
    "# cadCAD Model as Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Choose Distributed Execution Mode:\n",
    "**Simulation Execution Modes:**\n",
    "    `cadCAD` executes a process per System Model Configuration and a thread per System Simulation.\n",
    "\n",
    "**Class:** `cadCAD.engine.ExecutionMode`\n",
    "\n",
    "**Closed Source Attributes:**\n",
    "\n",
    "* **Distributed Mode:** concurrently executes simulations (runs) on AWS EMR cluster within multiple / n AWS worker \n",
    "instances per given System Model Configuration (Example: \n",
    "`cadCAD.engine.ExecutionMode().distributed`).\n",
    "multiple processes per given System Model Configuration (Example: `cadCAD.engine.ExecutionMode().distributed`).\n",
    "\n",
    "#### Value: \n",
    "This feature enables increased Memory, CPU speed, and storage capacity beyond users' local machines as well as the following \n",
    "capabilities. (**add Benchmark documentation here**)\n",
    "* Low latent Stochastic (Monte-Carlo) simulation & Agent-Based Modeling via distributed execution on AWS computing \n",
    "cluster\n",
    "* large simulated event-dataset generation and storage\n",
    "* System Modeling integration into distributed data pipelines for \"Big Data\" transformation\n",
    "\n",
    "##### [Open Source Attributes](https://github.com/BlockScience/cadCAD/blob/master/documentation/Simulation_Execution.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Execution Context using **Distributed** (Execution) Mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distributed_sims = distributed_simulations(transform)\n",
    "exec_mode = ExecutionMode()\n",
    "distributed_ctx = ExecutionContext(context=exec_mode.distributed, method=distributed_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create **Distributed** Simulation Executor:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = Executor(exec_context=distributed_ctx, configs=configs, spark_context=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. *Execute **Distributed** Simulation: Produce System Event Dataset*\n",
    "A Simulation execution produces a System Event Dataset and the Tensor Field applied to initial states used to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations Length: 500\n",
      "Execution Method: distroduce_proc\n",
      "Execution Mode: remote_distributed"
     ]
    }
   ],
   "source": [
    "rdd, tensor_fields, sessions = run.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD: Typless\n",
    "* Perfomance: \n",
    "    - Interactive (the cell below) - 98% Improvement\n",
    "    - Batch - 92% Improvement\n",
    "    - Micro-Batch (near Real-Time) (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result: list = list(rdd.collect())\n",
    "# If the user wants the result Data Structure for local use on the Master node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame:\n",
    "* Typefull Conversions of Datasets have the shortest distributed execution time on Big Data Transformation / Generation\n",
    "(Justification: Perfomance Optimization - https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/)\n",
    "* Typeless Conversions of Datasets have a significantly shorter distributed execution time than local execution but silghtly longer than Typefull Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Typefull Conversion: From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+----------+-------+-------------------+--------+\n",
      "|run| s1| s2| s3|  s4|simulation|substep|          timestamp|timestep|\n",
      "+---+---+---+---+----+----------+-------+-------------------+--------+\n",
      "|  1|0.0|0.0|1.0| 1.0|         0|      0|2018-10-01 15:16:24|       0|\n",
      "|  1|0.0|2.0|3.0| 4.0|         0|      1|2018-10-01 15:16:25|       1|\n",
      "|  1|2.0|2.0|3.0| 4.0|         0|      2|2018-10-01 15:16:25|       1|\n",
      "|  1|0.0|0.0|3.0| 4.0|         0|      3|2018-10-01 15:16:25|       1|\n",
      "|  1|0.0|2.0|3.0| 7.0|         0|      1|2018-10-01 15:16:26|       2|\n",
      "|  1|2.0|2.0|3.0| 7.0|         0|      2|2018-10-01 15:16:26|       2|\n",
      "|  1|0.0|0.0|3.0| 7.0|         0|      3|2018-10-01 15:16:26|       2|\n",
      "|  1|0.0|2.0|3.0|10.0|         0|      1|2018-10-01 15:16:27|       3|\n",
      "|  1|2.0|2.0|3.0|10.0|         0|      2|2018-10-01 15:16:27|       3|\n",
      "|  1|0.0|0.0|3.0|10.0|         0|      3|2018-10-01 15:16:27|       3|\n",
      "|  1|0.0|2.0|3.0|13.0|         0|      1|2018-10-01 15:16:28|       4|\n",
      "|  1|2.0|2.0|3.0|13.0|         0|      2|2018-10-01 15:16:28|       4|\n",
      "|  1|0.0|0.0|3.0|13.0|         0|      3|2018-10-01 15:16:28|       4|\n",
      "|  1|0.0|2.0|3.0| 2.0|         0|      1|2018-10-01 15:16:29|       5|\n",
      "|  1|2.0|2.0|3.0| 2.0|         0|      2|2018-10-01 15:16:29|       5|\n",
      "|  1|0.0|0.0|3.0| 2.0|         0|      3|2018-10-01 15:16:29|       5|\n",
      "|  1|0.0|2.0|3.0| 5.0|         0|      1|2018-10-01 15:16:30|       6|\n",
      "|  1|2.0|2.0|3.0| 5.0|         0|      2|2018-10-01 15:16:30|       6|\n",
      "|  1|0.0|0.0|3.0| 5.0|         0|      3|2018-10-01 15:16:30|       6|\n",
      "|  1|0.0|2.0|3.0| 8.0|         0|      1|2018-10-01 15:16:31|       7|\n",
      "|  1|2.0|2.0|3.0| 8.0|         0|      2|2018-10-01 15:16:31|       7|\n",
      "|  1|0.0|0.0|3.0| 8.0|         0|      3|2018-10-01 15:16:31|       7|\n",
      "|  1|0.0|2.0|3.0|11.0|         0|      1|2018-10-01 15:16:32|       8|\n",
      "|  1|2.0|2.0|3.0|11.0|         0|      2|2018-10-01 15:16:32|       8|\n",
      "|  1|0.0|0.0|3.0|11.0|         0|      3|2018-10-01 15:16:32|       8|\n",
      "|  1|0.0|2.0|3.0|14.0|         0|      1|2018-10-01 15:16:33|       9|\n",
      "|  1|2.0|2.0|3.0|14.0|         0|      2|2018-10-01 15:16:33|       9|\n",
      "|  1|0.0|0.0|3.0|14.0|         0|      3|2018-10-01 15:16:33|       9|\n",
      "|  1|0.0|2.0|3.0|17.0|         0|      1|2018-10-01 15:16:34|      10|\n",
      "|  1|2.0|2.0|3.0|17.0|         0|      2|2018-10-01 15:16:34|      10|\n",
      "|  1|0.0|0.0|3.0|17.0|         0|      3|2018-10-01 15:16:34|      10|\n",
      "|  1|0.0|2.0|3.0|20.0|         0|      1|2018-10-01 15:16:35|      11|\n",
      "|  1|2.0|2.0|3.0|20.0|         0|      2|2018-10-01 15:16:35|      11|\n",
      "|  1|0.0|0.0|3.0|20.0|         0|      3|2018-10-01 15:16:35|      11|\n",
      "|  1|0.0|2.0|3.0|23.0|         0|      1|2018-10-01 15:16:36|      12|\n",
      "|  1|2.0|2.0|3.0|23.0|         0|      2|2018-10-01 15:16:36|      12|\n",
      "|  1|0.0|0.0|3.0|23.0|         0|      3|2018-10-01 15:16:36|      12|\n",
      "|  1|0.0|2.0|3.0|26.0|         0|      1|2018-10-01 15:16:37|      13|\n",
      "|  1|2.0|2.0|3.0|26.0|         0|      2|2018-10-01 15:16:37|      13|\n",
      "|  1|0.0|0.0|3.0|26.0|         0|      3|2018-10-01 15:16:37|      13|\n",
      "|  1|0.0|2.0|3.0|29.0|         0|      1|2018-10-01 15:16:38|      14|\n",
      "|  1|2.0|2.0|3.0|29.0|         0|      2|2018-10-01 15:16:38|      14|\n",
      "|  1|0.0|0.0|3.0|29.0|         0|      3|2018-10-01 15:16:38|      14|\n",
      "|  1|0.0|2.0|3.0|32.0|         0|      1|2018-10-01 15:16:39|      15|\n",
      "|  1|2.0|2.0|3.0|32.0|         0|      2|2018-10-01 15:16:39|      15|\n",
      "|  1|0.0|0.0|3.0|32.0|         0|      3|2018-10-01 15:16:39|      15|\n",
      "|  1|0.0|2.0|3.0|35.0|         0|      1|2018-10-01 15:16:40|      16|\n",
      "|  1|2.0|2.0|3.0|35.0|         0|      2|2018-10-01 15:16:40|      16|\n",
      "|  1|0.0|0.0|3.0|35.0|         0|      3|2018-10-01 15:16:40|      16|\n",
      "|  1|0.0|2.0|3.0|38.0|         0|      1|2018-10-01 15:16:41|      17|\n",
      "|  1|2.0|2.0|3.0|38.0|         0|      2|2018-10-01 15:16:41|      17|\n",
      "|  1|0.0|0.0|3.0|38.0|         0|      3|2018-10-01 15:16:41|      17|\n",
      "|  1|0.0|2.0|3.0|41.0|         0|      1|2018-10-01 15:16:42|      18|\n",
      "|  1|2.0|2.0|3.0|41.0|         0|      2|2018-10-01 15:16:42|      18|\n",
      "|  1|0.0|0.0|3.0|41.0|         0|      3|2018-10-01 15:16:42|      18|\n",
      "|  1|0.0|2.0|3.0|44.0|         0|      1|2018-10-01 15:16:43|      19|\n",
      "|  1|2.0|2.0|3.0|44.0|         0|      2|2018-10-01 15:16:43|      19|\n",
      "|  1|0.0|0.0|3.0|44.0|         0|      3|2018-10-01 15:16:43|      19|\n",
      "|  1|0.0|2.0|3.0|47.0|         0|      1|2018-10-01 15:16:44|      20|\n",
      "|  1|2.0|2.0|3.0|47.0|         0|      2|2018-10-01 15:16:44|      20|\n",
      "|  1|0.0|0.0|3.0|47.0|         0|      3|2018-10-01 15:16:44|      20|\n",
      "|  1|0.0|2.0|3.0|50.0|         0|      1|2018-10-01 15:16:45|      21|\n",
      "|  1|2.0|2.0|3.0|50.0|         0|      2|2018-10-01 15:16:45|      21|\n",
      "|  1|0.0|0.0|3.0|50.0|         0|      3|2018-10-01 15:16:45|      21|\n",
      "|  1|0.0|2.0|3.0|53.0|         0|      1|2018-10-01 15:16:46|      22|\n",
      "|  1|2.0|2.0|3.0|53.0|         0|      2|2018-10-01 15:16:46|      22|\n",
      "|  1|0.0|0.0|3.0|53.0|         0|      3|2018-10-01 15:16:46|      22|\n",
      "|  1|0.0|2.0|3.0|56.0|         0|      1|2018-10-01 15:16:47|      23|\n",
      "|  1|2.0|2.0|3.0|56.0|         0|      2|2018-10-01 15:16:47|      23|\n",
      "|  1|0.0|0.0|3.0|56.0|         0|      3|2018-10-01 15:16:47|      23|\n",
      "|  1|0.0|2.0|3.0|59.0|         0|      1|2018-10-01 15:16:48|      24|\n",
      "|  1|2.0|2.0|3.0|59.0|         0|      2|2018-10-01 15:16:48|      24|\n",
      "|  1|0.0|0.0|3.0|59.0|         0|      3|2018-10-01 15:16:48|      24|\n",
      "|  1|0.0|2.0|3.0|62.0|         0|      1|2018-10-01 15:16:49|      25|\n",
      "|  1|2.0|2.0|3.0|62.0|         0|      2|2018-10-01 15:16:49|      25|\n",
      "|  1|0.0|0.0|3.0|62.0|         0|      3|2018-10-01 15:16:49|      25|\n",
      "|  1|0.0|2.0|3.0|65.0|         0|      1|2018-10-01 15:16:50|      26|\n",
      "|  1|2.0|2.0|3.0|65.0|         0|      2|2018-10-01 15:16:50|      26|\n",
      "|  1|0.0|0.0|3.0|65.0|         0|      3|2018-10-01 15:16:50|      26|\n",
      "|  1|0.0|2.0|3.0|68.0|         0|      1|2018-10-01 15:16:51|      27|\n",
      "|  1|2.0|2.0|3.0|68.0|         0|      2|2018-10-01 15:16:51|      27|\n",
      "|  1|0.0|0.0|3.0|68.0|         0|      3|2018-10-01 15:16:51|      27|\n",
      "|  1|0.0|2.0|3.0|71.0|         0|      1|2018-10-01 15:16:52|      28|\n",
      "|  1|2.0|2.0|3.0|71.0|         0|      2|2018-10-01 15:16:52|      28|\n",
      "|  1|0.0|0.0|3.0|71.0|         0|      3|2018-10-01 15:16:52|      28|\n",
      "|  1|0.0|2.0|3.0|74.0|         0|      1|2018-10-01 15:16:53|      29|\n",
      "|  1|2.0|2.0|3.0|74.0|         0|      2|2018-10-01 15:16:53|      29|\n",
      "|  1|0.0|0.0|3.0|74.0|         0|      3|2018-10-01 15:16:53|      29|\n",
      "|  1|0.0|2.0|3.0|77.0|         0|      1|2018-10-01 15:16:54|      30|\n",
      "|  1|2.0|2.0|3.0|77.0|         0|      2|2018-10-01 15:16:54|      30|\n",
      "|  1|0.0|0.0|3.0|77.0|         0|      3|2018-10-01 15:16:54|      30|\n",
      "|  1|0.0|2.0|3.0|80.0|         0|      1|2018-10-01 15:16:55|      31|\n",
      "|  1|2.0|2.0|3.0|80.0|         0|      2|2018-10-01 15:16:55|      31|\n",
      "|  1|0.0|0.0|3.0|80.0|         0|      3|2018-10-01 15:16:55|      31|\n",
      "|  1|0.0|2.0|3.0|83.0|         0|      1|2018-10-01 15:16:56|      32|\n",
      "|  1|2.0|2.0|3.0|83.0|         0|      2|2018-10-01 15:16:56|      32|\n",
      "|  1|0.0|0.0|3.0|83.0|         0|      3|2018-10-01 15:16:56|      32|\n",
      "|  1|0.0|2.0|3.0|86.0|         0|      1|2018-10-01 15:16:57|      33|\n",
      "|  1|2.0|2.0|3.0|86.0|         0|      2|2018-10-01 15:16:57|      33|\n",
      "|  1|0.0|0.0|3.0|86.0|         0|      3|2018-10-01 15:16:57|      33|\n",
      "+---+---+---+---+----+----------+-------+-------------------+--------+\n",
      "only showing top 100 rows"
     ]
    }
   ],
   "source": [
    "# Fastest\n",
    "sdf: DataFrame = to_spark_df(rdd, spark, sweep_config.genesis_states)\n",
    "sdf.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----------+-------+-------------------+--------+\n",
      "|run| s1| s2| s3| s4|simulation|substep|          timestamp|timestep|\n",
      "+---+---+---+---+---+----------+-------+-------------------+--------+\n",
      "|  1|0.0|0.0|1.0|1.0|         0|      0|2018-10-01 15:16:24|       0|\n",
      "|  1|0.0|2.0|3.0|4.0|         0|      1|2018-10-01 15:16:25|       1|\n",
      "+---+---+---+---+---+----------+-------+-------------------+--------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500500"
     ]
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45003000"
     ]
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45003000"
     ]
    }
   ],
   "source": [
    "# rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Typeless Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long, and wont succede if the resulting dataset is larger than the allocated memory\n",
    "# sdf: DataFrame = to_spark_df(rdd, spark)\n",
    "# sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Typefull Conversion: From Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   run   s1   s2   s3   s4  simulation  substep            timestamp  timestep\n",
      "0    1  0.0  0.0  1.0  1.0           0        0  2018-10-01 15:16:24         0\n",
      "1    1  0.0  2.0  3.0  4.0           0        1  2018-10-01 15:16:25         1\n",
      "2    1  2.0  2.0  3.0  4.0           0        2  2018-10-01 15:16:25         1\n",
      "3    1  0.0  0.0  3.0  4.0           0        3  2018-10-01 15:16:25         1\n",
      "4    1  0.0  2.0  3.0  7.0           0        1  2018-10-01 15:16:26         2\n",
      "/usr/local/lib64/python3.6/site-packages/pyarrow/util.py:39: FutureWarning: pyarrow.open_stream is deprecated as of 0.17.0, please use pyarrow.ipc.open_stream instead\n",
      "  warnings.warn(msg, FutureWarning)"
     ]
    }
   ],
   "source": [
    "# del pdf\n",
    "pdf = sdf.toPandas()\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Typefull Conversion: From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   run   s1   s2   s3   s4  simulation  substep            timestamp  timestep\n",
      "0    1  0.0  0.0  1.0  1.0           0        0  2018-10-01 15:16:24         0\n",
      "1    1  0.0  2.0  3.0  4.0           0        1  2018-10-01 15:16:25         1\n",
      "2    1  2.0  2.0  3.0  4.0           0        2  2018-10-01 15:16:25         1\n",
      "3    1  0.0  0.0  3.0  4.0           0        3  2018-10-01 15:16:25         1\n",
      "4    1  0.0  2.0  3.0  7.0           0        1  2018-10-01 15:16:26         2\n",
      "/usr/local/lib64/python3.6/site-packages/pyarrow/util.py:39: FutureWarning: pyarrow.open_stream is deprecated as of 0.17.0, please use pyarrow.ipc.open_stream instead\n",
      "  warnings.warn(msg, FutureWarning)"
     ]
    }
   ],
   "source": [
    "del pdf\n",
    "pdf: pd.DataFrame = to_pandas_df(rdd, sweep_config.genesis_states)\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Typeless Conversion: From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    s1   s2   s3   s4            timestamp  simulation  run  substep  timestep\n",
      "0  0.0  0.0  1.0  1.0  2018-10-01 15:16:24           0    1        0         0\n",
      "1  0.0  2.0  3.0  4.0  2018-10-01 15:16:25           0    1        1         1\n",
      "2  2.0  2.0  3.0  4.0  2018-10-01 15:16:25           0    1        2         1\n",
      "3  0.0  0.0  3.0  4.0  2018-10-01 15:16:25           0    1        3         1\n",
      "4  0.0  2.0  3.0  7.0  2018-10-01 15:16:26           0    1        1         2"
     ]
    }
   ],
   "source": [
    "del pdf\n",
    "pdf: pd.DataFrame = to_pandas_df(rdd)\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Simulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = set_write_path(sc, user, 'data/param_sweep_df')\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Overwrite\n",
    "file_path = set_write_path(SC, user, 'data/param_sweep_rdd')\n",
    "rdd.saveAsTextFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read Simulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+-------------------+----------+---+-------+--------+\n",
      "| s1| s2| s3|  s4|          timestamp|simulation|run|substep|timestep|\n",
      "+---+---+---+----+-------------------+----------+---+-------+--------+\n",
      "|2.0|2.0|3.0|14.0|2018-10-01 15:16:33|         0|  2|      2|       9|\n",
      "|0.0|0.0|3.0|14.0|2018-10-01 15:16:33|         0|  2|      3|       9|\n",
      "|0.0|2.0|3.0|17.0|2018-10-01 15:16:34|         0|  2|      1|      10|\n",
      "|2.0|2.0|3.0|17.0|2018-10-01 15:16:34|         0|  2|      2|      10|\n",
      "|0.0|0.0|3.0|17.0|2018-10-01 15:16:34|         0|  2|      3|      10|\n",
      "+---+---+---+----+-------------------+----------+---+-------+--------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(file_path)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          0\n",
      "0         {'s1': 0.0, 's2': 0.0, 's3': 1.0, 's4': 1.0, '...\n",
      "1         {'s1': 0, 's2': 2, 'timestamp': '2018-10-01 15...\n",
      "2         {'s1': 2, 's2': 2, 'timestamp': '2018-10-01 15...\n",
      "3         {'s1': 0, 's2': 0, 'timestamp': '2018-10-01 15...\n",
      "4         {'s1': 0, 's2': 2, 'timestamp': '2018-10-01 15...\n",
      "...                                                     ...\n",
      "45002995  {'s1': 5, 's2': 5, 'timestamp': '2018-10-01 16...\n",
      "45002996  {'s1': 0, 's2': 0, 'timestamp': '2018-10-01 16...\n",
      "45002997  {'s1': 0, 's2': 5, 'timestamp': '2018-10-01 16...\n",
      "45002998  {'s1': 5, 's2': 5, 'timestamp': '2018-10-01 16...\n",
      "45002999  {'s1': 0, 's2': 0, 'timestamp': '2018-10-01 16...\n",
      "\n",
      "[45003000 rows x 1 columns]"
     ]
    }
   ],
   "source": [
    "# Specify file path\n",
    "read_rdd = sc.textFile(file_path)\n",
    "# Get result\n",
    "rdd_result = read_rdd.collect()\n",
    "pdf_from_rdd = pd.DataFrame(rdd_result)\n",
    "del rdd_result\n",
    "pdf_from_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations: Returned as List of Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<cadCAD.configuration.Configuration object at 0x116a21b90>,\n",
      " <cadCAD.configuration.Configuration object at 0x116a21c90>]\n",
      "\n",
      "{'M': {'alpha': 1, 'beta': 2, 'gamma': 3, 'omega': 7},\n",
      " 'N': 2,\n",
      " 'T': range(0, 10),\n",
      " 'run_id': 1,\n",
      " 'simulation_id': 0}\n"
     ]
    }
   ],
   "source": [
    "fmt_configs = configs_as_objs(configs)\n",
    "pprint(fmt_configs)\n",
    "print()\n",
    "pprint(fmt_configs[0].sim_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations: Returned as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>simulation_id</th>\n",
       "      <th>run_id</th>\n",
       "      <th>sim_config</th>\n",
       "      <th>initial_state</th>\n",
       "      <th>seeds</th>\n",
       "      <th>env_processes</th>\n",
       "      <th>exogenous_states</th>\n",
       "      <th>partial_state_updates</th>\n",
       "      <th>policy_ops</th>\n",
       "      <th>kwargs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cadCAD_user=0_1</td>\n",
       "      <td>cadCAD_user</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'N': 2, 'T': (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), ...</td>\n",
       "      <td>{'s1': 0.0, 's2': 0.0, 's3': 1.0, 's4': 1.0, '...</td>\n",
       "      <td>{'z': RandomState(MT19937), 'a': RandomState(M...</td>\n",
       "      <td>{'s3': [&lt;function &lt;lambda&gt; at 0x116a23b00&gt;, &lt;f...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[{'policies': {'p1': &lt;function p1m1 at 0x116a1...</td>\n",
       "      <td>[&lt;function &lt;lambda&gt; at 0x116867d40&gt;]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cadCAD_user=1_1</td>\n",
       "      <td>cadCAD_user</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'N': 2, 'T': (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), ...</td>\n",
       "      <td>{'s1': 0.0, 's2': 0.0, 's3': 1.0, 's4': 1.0, '...</td>\n",
       "      <td>{'z': RandomState(MT19937), 'a': RandomState(M...</td>\n",
       "      <td>{'s3': [&lt;function &lt;lambda&gt; at 0x116a23b00&gt;, &lt;f...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[{'policies': {'p1': &lt;function p1m1 at 0x116a1...</td>\n",
       "      <td>[&lt;function &lt;lambda&gt; at 0x116867d40&gt;]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id      user_id  simulation_id  run_id  \\\n",
       "0  cadCAD_user=0_1  cadCAD_user              0       1   \n",
       "1  cadCAD_user=1_1  cadCAD_user              1       1   \n",
       "\n",
       "                                          sim_config  \\\n",
       "0  {'N': 2, 'T': (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), ...   \n",
       "1  {'N': 2, 'T': (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), ...   \n",
       "\n",
       "                                       initial_state  \\\n",
       "0  {'s1': 0.0, 's2': 0.0, 's3': 1.0, 's4': 1.0, '...   \n",
       "1  {'s1': 0.0, 's2': 0.0, 's3': 1.0, 's4': 1.0, '...   \n",
       "\n",
       "                                               seeds  \\\n",
       "0  {'z': RandomState(MT19937), 'a': RandomState(M...   \n",
       "1  {'z': RandomState(MT19937), 'a': RandomState(M...   \n",
       "\n",
       "                                       env_processes exogenous_states  \\\n",
       "0  {'s3': [<function <lambda> at 0x116a23b00>, <f...               {}   \n",
       "1  {'s3': [<function <lambda> at 0x116a23b00>, <f...               {}   \n",
       "\n",
       "                               partial_state_updates  \\\n",
       "0  [{'policies': {'p1': <function p1m1 at 0x116a1...   \n",
       "1  [{'policies': {'p1': <function p1m1 at 0x116a1...   \n",
       "\n",
       "                             policy_ops kwargs  \n",
       "0  [<function <lambda> at 0x116867d40>]     {}  \n",
       "1  [<function <lambda> at 0x116867d40>]     {}  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_df = configs_as_dataframe(configs)\n",
    "configs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations: Returned as List of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_processes': {'s3': [<function <lambda> at 0x7f8f9c99bd90>,\n",
      "                          <function <lambda> at 0x7f8f9c9a11e0>],\n",
      "                   's4': <function env_trigger.<locals>.trigger.<locals>.env_update at 0x7f8f9c9a12f0>},\n",
      " 'exogenous_states': {},\n",
      " 'initial_state': {'s1': 0.0,\n",
      "                   's2': 0.0,\n",
      "                   's3': 1.0,\n",
      "                   's4': 1.0,\n",
      "                   'timestamp': '2018-10-01 15:16:24'},\n",
      " 'kwargs': {},\n",
      " 'partial_state_updates': [{'policies': {'p1': <function p1m1 at 0x7f8f9c985ea0>,\n",
      "                                         'p2': <function p2m1 at 0x7f8f9c985f28>},\n",
      "                            'variables': {'s1': <function s1m1 at 0x7f8f9c99b268>,\n",
      "                                          's2': <function s2m1 at 0x7f8f9c99b2f0>,\n",
      "                                          's3': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99bae8>,\n",
      "                                          's4': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99bea0>,\n",
      "                                          'timestamp': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99b730>}},\n",
      "                           {'policies': {'p1': <function p1m2 at 0x7f8f9c99b048>,\n",
      "                                         'p2': <function p2m2 at 0x7f8f9c99b0d0>},\n",
      "                            'variables': {'s1': <function s1m2 at 0x7f8f9c99b378>,\n",
      "                                          's2': <function s2m2 at 0x7f8f9c99b400>,\n",
      "                                          's3': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99bbf8>,\n",
      "                                          's4': <function var_trigger.<locals>.<lambda> at 0x7f8f9c9a1048>,\n",
      "                                          'timestamp': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99b840>}},\n",
      "                           {'policies': {'p1': <function p1m3 at 0x7f8f9c99b158>,\n",
      "                                         'p2': <function p2m3 at 0x7f8f9c99b1e0>},\n",
      "                            'variables': {'s1': <function s1m3 at 0x7f8f9c99b488>,\n",
      "                                          's2': <function s2m3 at 0x7f8f9c99b510>,\n",
      "                                          's3': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99bd08>,\n",
      "                                          's4': <function var_trigger.<locals>.<lambda> at 0x7f8f9c9a1158>,\n",
      "                                          'timestamp': <function var_trigger.<locals>.<lambda> at 0x7f8f9c99b950>}}],\n",
      " 'policy_ops': [<function <lambda> at 0x7f8f9cb39158>],\n",
      " 'run_id': 249,\n",
      " 'seeds': {'a': <mtrand.RandomState object at 0x7f8f9c9a21f8>,\n",
      "           'b': <mtrand.RandomState object at 0x7f8f9c9a2240>,\n",
      "           'c': <mtrand.RandomState object at 0x7f8f9c9a2288>,\n",
      "           'z': <mtrand.RandomState object at 0x7f8f9ca04948>},\n",
      " 'session_id': 'cadCAD_user=0_249',\n",
      " 'sim_config': {'M': {'alpha': 1, 'beta': 2, 'gamma': 3, 'omega': 7},\n",
      "                'N': 250,\n",
      "                'T': range(0, 5000),\n",
      "                'run_id': 249,\n",
      "                'simulation_id': 0},\n",
      " 'simulation_id': 0,\n",
      " 'user_id': 'cadCAD_user'}"
     ]
    }
   ],
   "source": [
    "configs_dicts: list = configs_as_dicts(configs)\n",
    "pprint(configs_dicts[0]['sim_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
